# -*- coding: utf-8 -*-
"""
@author: Yinghao Li
"""

from core import LandmarkMapper, glm, utils, Mesh
import numpy as np
import cv2
import os
from enum import IntEnum
from typing import Optional, List


class Vertex:
    """
    A representation for a vertex during rendering, used internally.

    It's used to build the vertices that will be rendered, and new vertices that are generated by the renderer
    (during clipping).
    I should check at some point that no unnecessary copies of the vertex data is created in some places, but I
    think it's pretty much fine.

    FragmentShader and SoftwareRenderer use this.

    This is the same as the one in the current render_detail.hpp, except that this is fully templated.

    """
    def __init__(self, position=None, color=None, texcoords=None):
        self.position = position  # XYZW
        self.color = color  # RGB order
        self.texcoords = texcoords  # UV


class TriangleToRasterize:
    """
    A representation for a triangle that is to be rasterised.
    Stores the enclosing bounding box of the triangle that is calculated during rendering and used during rasterisation.

    Used in render_affine and render.
    """
    def __init__(self, v0: Vertex, v1: Vertex, v2: Vertex, min_x: int, max_x: int, min_y: int, max_y: int):
        self.v0 = v0
        self.v1 = v1
        self.v2 = v2
        self.min_x = min_x
        self.min_y = min_y
        self.max_x = max_x
        self.max_y = max_y


class TextureInterpolation(IntEnum):
    """
    Just realize # define function in C++
    """
    NearestNeighbour = 1
    Bilinear = 2
    Area = 3


def implicit_line(x: float, y: float, v1: np.ndarray, v2: np.ndarray) -> float:
    return (v1[1] - v2[1]) * x + (v2[0] + v1[0]) * y + v1[0] * v2[1] - v2[0] * v1[1]


def calculate_affine_z_direction(affine_camera_matrix: np.ndarray) -> np.ndarray:
    """
    Takes a 3x4 affine camera matrix estimated with fitting::estimate_affine_camera
    and computes the cross product of the first two rows to create a third axis that
    is orthogonal to the first two.
    This allows us to produce z values and figure out correct depth ordering in the
    rendering and for texture extraction.

    :param affine_camera_matrix: A 3x4 affine camera matrix.
    :return: The matrix with a third row inserted.
    """
    # Take the cross product of row 0 with row 1 to get the direction perpendicular to the viewing plane
    # (= the viewing direction).
    # Todo: We should check if we look/project into the right direction - the sign could be wrong?
    affine_cam_z_rotation = np.cross(affine_camera_matrix[0, :3], affine_camera_matrix[1, :3])
    # The 4th component is zero, so it does not influence the normalisation. ???
    affine_cam_z_rotation /= np.linalg.norm(affine_cam_z_rotation)

    affine_cam_4x4 = np.eye(4)
    affine_cam_4x4[:2, :] = affine_camera_matrix[:2, :]
    affine_cam_4x4[2, :3] = affine_cam_z_rotation
    return affine_cam_4x4


def calculate_clipped_bounding_box(v0: np.ndarray, v1: np.ndarray, v2: np.ndarray,
                                   viewport_width: int, viewport_height: int) -> list:
    """
    Calculates the enclosing bounding box of 3 vertices (a triangle). If the
    triangle is partly outside the screen, it will be clipped appropriately.

    注意，这里的输出不是HPP文件中的Rect类型，而是一个列表，元素为Xmin, Ymin, Xmax, Ymax，即左上角和右下角坐标。

    Todo: If it is fully outside the screen, check what happens, but it works.

    :param v0: First vertex.
    :param v1: Second vertex.
    :param v2: Third vertex.
    :param viewport_width: Screen width.
    :param viewport_height: Screen height.
    :return: A bounding box rectangle.
    """
    v = np.array([v0, v1, v2], copy=False)
    minx = int(np.min(v, axis=0)[0])
    miny = int(np.min(v, axis=0)[1])
    maxx = int(np.min([np.max(v, axis=0)[0], viewport_width]))
    maxy = int(np.min([np.max(v, axis=0)[1], viewport_height]))
    return [minx, miny, maxx, maxy]


def are_vertices_ccw_in_screen_space(v0: np.ndarray, v1: np.ndarray, v2: np.ndarray) -> bool:
    """
    Computes whether the triangle formed out of the given three vertices is
    counter-clockwise in screen space. Assumes the origin of the screen is on
    the top-left, and the y-axis goes down (as in OpenCV images).

    :param v0: First vertex.
    :param v1: Second vertex.
    :param v2: Third vertex.
    :return: Whether the vertices are CCW in screen space.
    """
    dx01 = v1[0] - v0[0]
    dy01 = v1[1] - v0[1]
    dx02 = v2[0] - v0[0]
    dy02 = v2[1] - v0[1]
    return dx01 * dy02 - dy01 * dx02 < 0


def raster_triangle_affine(triangle: TriangleToRasterize, colorbuffer: np.ndarray, depthbuffer: np.ndarray):
    """
    Rasters a triangle into the given colour and depth buffer.

    In essence, loop through the pixels inside the triangle's bounding box, calculate the barycentric coordinates,
    and if inside the triangle and the z-test is passed, then draw the point using the barycentric coordinates for
    colour interpolation.

    Does not do perspective-correct weighting, and therefore only works with the affine rendering pipeline.

    No texturing at the moment.

    Note: See where and how this is used, and how similar it is to the "normal" raster_triangle.
    Maybe rename to raster_triangle_vertexcolour?

    :param triangle: A triangle.
    :param colorbuffer: The colour buffer to draw into.
    :param depthbuffer: The depth buffer to draw into and use for the depth test.
    :return: None
    """
    for yi in range(triangle.min_y, triangle.max_y + 1):
        for xi in range(triangle.min_x, triangle.max_x + 1):
            # we want centers of pixels to be used in computations.
            x = xi + 0.5
            y = yi + 0.5

            # these will be used for barycentric weights computation
            one_over_v0_to_line12 = 1 / implicit_line(triangle.v0.position[0], triangle.v0.position[1],
                                                      triangle.v1.position, triangle.v2.position)
            one_over_v1_to_line20 = 1 / implicit_line(triangle.v1.position[0], triangle.v1.position[1],
                                                      triangle.v2.position, triangle.v0.position)
            one_over_v2_to_line01 = 1 / implicit_line(triangle.v2.position[0], triangle.v2.position[1],
                                                      triangle.v0.position, triangle.v1.position)

            # affine barycentric weights
            alpha = implicit_line(x, y, triangle.v1.position, triangle.v2.position) * one_over_v0_to_line12
            beta = implicit_line(x, y, triangle.v2.position, triangle.v0.position) * one_over_v1_to_line20
            gamma = implicit_line(x, y, triangle.v0.position, triangle.v1.position) * one_over_v2_to_line01

            # if pixel (x, y) is inside the triangle or on one of its edges
            if alpha >= 0 and beta >= 0 and gamma >= 0 and 1 - alpha - beta - gamma < 0.001:
                pixel_index_row = yi
                pixel_index_col = xi

                z_affine = alpha * triangle.v0.position[2] + triangle.v1.position[2] + triangle.v2.position[2]
                if z_affine > depthbuffer[pixel_index_row, pixel_index_col]:
                    continue
                # attributes interpolation
                # pixel_color is in RGB, v.color are RGB
                pixel_color = alpha * triangle.v0.color + beta * triangle.v1.color + gamma * triangle.v2.color

                # clamp bytes to 255
                red = 255 * min(pixel_color[0], 1)
                green = 255 * min(pixel_color[1], 1)
                blue = 255 * min(pixel_color[2], 1)

                colorbuffer[pixel_index_row, pixel_index_col, 0] = blue
                colorbuffer[pixel_index_row, pixel_index_col, 1] = green
                colorbuffer[pixel_index_row, pixel_index_col, 2] = red
                colorbuffer[pixel_index_row, pixel_index_col, 3] = 255
                depthbuffer[pixel_index_row, pixel_index_col] = z_affine
    return None


def render_affine(mesh: Mesh.Mesh, affine_camera_matrix: np.ndarray, viewport_width: int,
                  viewport_height: int, do_backface_culling: bool = True) -> tuple:
    """
    Renders the mesh using the given affine camera matrix and returns the colour and depth buffer images.
    The camera matrix should be one estimated with fitting::estimate_affine_camera (Hartley & Zisserman algorithm).

    If the given mesh is a shape-only mesh without vertex-colour information, the vertices will be rendered in grey.

    Todo: May consider an overload where we pass in an image, use that as colourbuffer and draw over it.
    Todo: Add texture rendering to this. Then, create an additional function in extract_texure that is fully
    Todo: optimised for only the extraction.

    Args:
        mesh:
            A 3D mesh.
        affine_camera_matrix:
            3x4 affine camera matrix.
        viewport_width:
            Screen width.
        viewport_height:
            Screen height.
        do_backface_culling:
            Whether the renderer should perform backface culling.

    Returns:
        A pair with the colourbuffer as its first element and the depthbuffer as the second element.
    """
    # The number of vertices has to be equal for both shape and colour, or,
    # alternatively, it has to be a shape-only model.
    assert len(mesh.vertices) == len(mesh.colors) or len(mesh.colors) == 0

    colorbuffer = np.zeros([viewport_height, viewport_width, 4], dtype=np.uint8)
    depthbuffer = np.zeros([viewport_height, viewport_width], dtype=np.float16)

    affine_with_z = calculate_affine_z_direction(affine_camera_matrix)

    projected_vertices = []
    for i in range(len(mesh.vertices)):
        vertex_screen_coords = affine_with_z.dot(np.hstack([mesh.vertices[i], 1.0]))
        if not len(mesh.colors):
            vertex_color = np.array([0.5, 0.5, 0.5])
        else:
            vertex_color = mesh.colors[i]
        projected_vertices.append(Vertex(vertex_screen_coords, vertex_color, mesh.texcoords[i]))

    # All vertices are screen-coordinates now
    for tri_indices in mesh.tvi:
        if do_backface_culling:
            if not are_vertices_ccw_in_screen_space(projected_vertices[tri_indices[0]].position,
                                                    projected_vertices[tri_indices[1]].position,
                                                    projected_vertices[tri_indices[2]].position):
                continue
        # Xmin, Ymin, Xmax, Ymax
        bounding_box = calculate_clipped_bounding_box(
            projected_vertices[tri_indices[0]].position, projected_vertices[tri_indices[1]].position,
            projected_vertices[tri_indices[2]].position, viewport_width, viewport_height)
        if bounding_box[2] <= bounding_box[0] or bounding_box[3] <= bounding_box[1]:
            continue

        t = TriangleToRasterize(projected_vertices[0], projected_vertices[1], projected_vertices[2],
                                bounding_box[0], bounding_box[2], bounding_box[1], bounding_box[3])

        raster_triangle_affine(t, colorbuffer, depthbuffer)
    return colorbuffer, depthbuffer


def is_triangle_visible(v0: np.ndarray, v1: np.ndarray, v2: np.ndarray, depthbuffer: np.ndarray) -> bool:
    """
    Checks whether all pixels in the given triangle are visible and
    returns true if and only if the whole triangle is visible.
    The vertices should be given in screen coordinates, but with their
    z-values preserved, so they can be compared against the depthbuffer.

    Obviously the depthbuffer given should have been created with the same projection
    matrix than the texture extraction is called with.

    Also, we don't do perspective-correct interpolation here I think, so only
    use it with affine and orthographic projection matrices.

    :param v0: First vertex, in screen coordinates (but still with their z-value).
    :param v1: Second vertex.
    :param v2: Third vertex.
    :param depthbuffer: Pre-calculated depthbuffer.
    :return: True if the whole triangle is visible in the image.
    """
    # Todo: Actually, only check the 3 vertex points, don't loop over the pixels - this should be enough.
    viewport_width = np.shape(depthbuffer)[1]
    viewport_height = np.shape(depthbuffer)[0]

    # Well, in in principle, we'd have to do the whole stuff as in render(), like clipping against the frustums etc.
    # But as long as our model is fully on the screen, we're fine. Todo: Doublecheck that.
    if not are_vertices_ccw_in_screen_space(v0, v1, v2):
        return False

    minx, miny, maxx, maxy = calculate_clipped_bounding_box(v0, v1, v2, viewport_width, viewport_height)

    whole_triangle_is_visible = True
    for yi in range(miny, maxy + 1):
        for xi in range(minx, maxx + 1):
            # we want centers of pixels to be used in computations. Do we?
            x = xi + 0.5
            y = yi + 0.5
            # these will be used for barycentric weights computation
            one_over_v0_to_line12 = 1 / implicit_line(v0[0], v0[1], v1, v2)
            one_over_v1_to_line20 = 1 / implicit_line(v1[0], v1[1], v2, v0)
            one_over_v2_to_line01 = 1 / implicit_line(v2[0], v2[1], v0, v1)

            # affine barycentric weights
            alpha = implicit_line(x, y, v1, v2) * one_over_v0_to_line12
            beta = implicit_line(x, y, v2, v0) * one_over_v1_to_line20
            gamma = implicit_line(x, y, v0, v1) * one_over_v2_to_line01

            # if pixel(x, y) is inside the triangle or on one of its edges
            if alpha >= 0 and beta >= 0 and gamma >= 0:
                z_affine = alpha * v0[2] + beta * v1[2] + gamma * v2[2]
                if z_affine < depthbuffer[yi, xi]:
                    whole_triangle_is_visible = False
                    break
        if not whole_triangle_is_visible:
            break
    if not whole_triangle_is_visible:
        return False
    return True


def is_point_in_triangle(point: np.ndarray, triv0: np.ndarray, triv1: np.ndarray, triv2: np.ndarray) -> bool:
    """
    Computes whether the given point is inside (or on the border of) the triangle
    formed out of the given three vertices.

    :param point: The point to check.
    :param triv0: First vertex.
    :param triv1: Second vertex.
    :param triv2: Third vertex.
    :return: Whether the point is inside the triangle.
    """
    # See http://www.blackpawn.com/texts/pointinpoly/
    # Compute vectors
    v0 = triv2 - triv0
    v1 = triv1 - triv0
    v2 = point - triv0

    # Compute dot products
    dot00 = v0.dot(v0)
    dot01 = v0.dot(v1)
    dot02 = v0.dot(v2)
    dot11 = v1.dot(v1)
    dot12 = v1.dot(v2)

    # Compute barycentric coordinates
    inv_denom = 1 / (dot00 * dot11 - dot01 * dot01)
    u = (dot11 * dot02 - dot01 * dot12) * inv_denom
    v = (dot00 * dot12 - dot01 * dot02) * inv_denom

    # Check if point is in triangle
    return (u >= 0) and (v >= 0) and (u + v < 1)


def get_affine_transform(src: np.ndarray, dst: np.ndarray) -> np.ndarray:
    """
    This function is copied from OpenCV,	originally under BSD licence.
    imgwarp.cpp from OpenCV-3.2.0.

    Calculates coefficients of affine transformation which maps (xi,yi) to (ui,vi), (i=1,2,3):

    ui = c00*xi + c01*yi + c02
    vi = c10*xi + c11*yi + c12

    Coefficients are calculated by solving linear system:
    / x0 y0  1  0  0  0 \ /c00\ /u0\
    | x1 y1  1  0  0  0 | |c01| |u1|
    | x2 y2  1  0  0  0 | |c02| |u2|
    |  0  0  0 x0 y0  1 | |c10| |v0|
    |  0  0  0 x1 y1  1 | |c11| |v1|
    \  0  0  0 x2 y2  1 / |c12| |v2|
    :param src: x, y
    :param dst: u, v
    :return: transform function
    """
    assert len(src) == len(dst) and len(src) == 3

#    a = np.zeros([6, 6])
#    b = np.zeros(6)

#    a[0::2, :3] = np.hstack([src, np.ones([3, 1])])
#    a[1::2, 3:] = np.hstack([src, np.ones([3, 1])])
#    b = b.reshape([6])

#    x = np.linalg.lstsq(a, b)[0]
#    x = np.reshape(x, (2, 3))
    pts1 = np.float32([[src[0][0], src[0][1]], [src[1][0], src[1][1]], [src[2][0], src[2][1]]])
    pts2 = np.float32([[dst[0][0], dst[0][1]], [dst[1][0], dst[1][1]], [dst[2][0], dst[2][1]]])
    x = cv2.getAffineTransform(pts1, pts2)
    return x


def extract_tecture(mesh: Mesh.Mesh, affine_camera_matrix: np.ndarray, image: np.ndarray,
                    compute_view_angle: bool = False, mapping_type: TextureInterpolation =
                    TextureInterpolation['NearestNeighbour'], isomap_resolution: int = 512) -> np.ndarray:
    """
    Extracts the texture of the face from the given image
    and stores it as isomap (a rectangular texture map).

    Note: Only use TextureInterpolation::NearestNeighbour for the moment, the other methods don't have
    correct handling of the alpha channel (and will most likely throw an exception).

    Todo: These should be renamed to extract_texture_affine? Can we combine both cases somehow?
    TODO: Or an overload with RenderingParameters?

    For TextureInterpolation::NearestNeighbour, returns a 4-channel isomap
    with the visibility in the 4th channel (0=invis, 255=visible).

    :param mesh: A mesh with texture coordinates.
    :param affine_camera_matrix: An estimated 3x4 affine camera matrix.
    :param image: The image to extract the texture from. Should be 8UC3, other types not supported yet.
    :param compute_view_angle: A flag whether the view angle of each vertex should be computed and returned. If set to
        true, the angle will be encoded into the alpha channel (0 meaning occluded or facing away 90? 127 meaning facing
        a 45?angle and 255 meaning front-facing, and all values in between). If set to false, the alpha channel will
        only contain 0 for occluded vertices and 255 for visible vertices.
    :param mapping_type: The interpolation type to be used for the extraction.
    :param isomap_resolution: The resolution of the generated isomap. Defaults to 512x512.
    :return: The extracted texture as isomap (texture map).
    """
    _, depthbuffer = render_affine(mesh, affine_camera_matrix, np.shape(image)[1], np.shape(image)[0])
    assert len(mesh.vertices) == len(mesh.texcoords)

    affine_camera_matrix_with_z = calculate_affine_z_direction(affine_camera_matrix)

    # Todo: We should handle gray images, but output a 4-channel isomap nevertheless I think.
    isomap = np.zeros([isomap_resolution, isomap_resolution, 4], dtype=np.uint8)
    for triangle_indices in mesh.tvi:
        # Note: If there's a performance problem, there's no need to capture the whole mesh - we could
        # capture only the three required vertices with their texcoords.
        # Find out if the current triangle is visible:
        # We do a second rendering-pass here. We use the depth-buffer of the final image, and then, here,
        # check if each pixel in a triangle is visible. If the whole triangle is visible, we use it to
        # extract the texture.
        # Possible improvement: - If only part of the triangle is visible, split it.
        v0_0 = np.hstack([mesh.vertices[triangle_indices[0]], 1.0])
        v1_0 = np.hstack([mesh.vertices[triangle_indices[1]], 1.0])
        v2_0 = np.hstack([mesh.vertices[triangle_indices[2]], 1.0])

        v0 = affine_camera_matrix_with_z.dot(v0_0)
        v1 = affine_camera_matrix_with_z.dot(v1_0)
        v2 = affine_camera_matrix_with_z.dot(v2_0)

        if not is_triangle_visible(v0, v1, v2, depthbuffer):
            continue

        if compute_view_angle:
            # Calculate how well visible the current triangle is:
            # (in essence, the dot product of the viewing direction (0, 0, 1) and the face normal)
            face_normal = utils.compute_face_normal(v0_0, v1_0, v2_0)[:3]
            # Transform the normal to "screen" (kind of "eye") space using the upper 3x3 part of the
            # affine camera matrix (=the translation can be ignored):
            face_normal_transformed = affine_camera_matrix_with_z[:3, :3].dot(face_normal)
            face_normal_transformed /= np.linalg.norm(face_normal_transformed)
            # Implementation notes regarding the affine camera matrix and the sign:
            # If the matrix given were the model_view matrix, the sign would be correct.
            # However, affine_camera_matrix includes glm::ortho, which includes a z-flip.
            # So we need to flip one of the two signs.
            # * viewing_direction(0.0f, 0.0f, 1.0f) is correct if affine_camera_matrix were only a model_view matrix
            # * affine_camera_matrix includes glm::ortho, which flips z, so we flip the sign of viewing_direction.
            # We don't need the dot product since viewing_direction.xy are 0 and .z is 1:
            angle = -face_normal_transformed[2]
            assert -1 <= angle <= 1
            if angle < 0:
                alpha_value = 0
            else:
                alpha_value = angle * 255
        else:
            alpha_value = 255

        # TODO: 这里可以进行优化
        src_tri = np.empty([3, 2])
        dst_tri = np.empty([3, 2])

        vec = np.hstack([mesh.vertices[triangle_indices[0]], 1])
        res = affine_camera_matrix_with_z.dot(vec)
        src_tri[0] = res[:2]

        vec = np.hstack([mesh.vertices[triangle_indices[1]], 1])
        res = affine_camera_matrix_with_z.dot(vec)
        src_tri[1] = res[:2]

        vec = np.hstack([mesh.vertices[triangle_indices[2]], 1])
        res = affine_camera_matrix_with_z.dot(vec)
        src_tri[2] = res[:2]

        dst_tri[0] = np.array([np.shape(isomap)[1] - 0.5,
                               np.shape(isomap)[0] - 0.5]) * mesh.texcoords[triangle_indices[0]]
        dst_tri[1] = np.array([np.shape(isomap)[1] - 0.5,
                               np.shape(isomap)[0] - 0.5]) * mesh.texcoords[triangle_indices[1]]
        dst_tri[2] = np.array([np.shape(isomap)[1] - 0.5,
                               np.shape(isomap)[0] - 0.5]) * mesh.texcoords[triangle_indices[2]]

        # We now have the source triangles in the image and the source triangle in the isomap
        # We use the inverse/ backward mapping approach, so we want to find the corresponding texel
        # (texture-pixel) for each pixel in the isomap

        # Get the inverse Affine Transform from original image: from dst (pixel in isomap) to src (in image)
        warp_mat_org_inv = get_affine_transform(dst_tri, src_tri)

        # We now loop over all pixels in the triangle and select, depending on the mapping type, the
        # corresponding texel(s) in the source image
        for x in range(int(np.min(dst_tri[:, 0])), int(np.max(dst_tri[:, 0]))):
            for y in range(int(np.min(dst_tri[:, 1])), int(np.max(dst_tri[:, 1]))):
                if is_point_in_triangle(np.array([x, y]), dst_tri[0], dst_tri[1], dst_tri[2]):
                    # As the coordinates of the transformed pixel in the image will most likely not lie
                    # on a texel, we have to choose how to calculate the pixel colors depending on the next texels

                    # There are three different texture interpolation methods: area, bilinear and nearest neighbour

                    # Area mapping: calculate mean color of texels in transformed pixel area
                    if mapping_type == TextureInterpolation['Area']:
                        # calculate positions of 4 corners of pixel in image (src)
                        homogenous_dst_upper_left = np.array([x - 0.5, y - 0.5, 1.0])
                        homogenous_dst_upper_right = np.array([x + 0.5, y - 0.5, 1.0])
                        homogenous_dst_lower_left = np.array([x - 0.5, y + 0.5, 1.0])
                        homogenous_dst_lower_right = np.array([x + 0.5, y + 0.5, 1.0])

                        src_texel_upper_left = warp_mat_org_inv.dot(homogenous_dst_upper_left)
                        src_texel_upper_right = warp_mat_org_inv.dot(homogenous_dst_upper_right)
                        src_texel_lower_left = warp_mat_org_inv.dot(homogenous_dst_lower_left)
                        src_texel_lower_right = warp_mat_org_inv.dot(homogenous_dst_lower_right)

                        min_a = np.min([src_texel_upper_left[0], src_texel_upper_right[0],
                                        src_texel_lower_left[0], src_texel_lower_right[0]])
                        max_a = np.max([src_texel_upper_left[0], src_texel_upper_right[0],
                                        src_texel_lower_left[0], src_texel_lower_right[0]])
                        min_b = np.min([src_texel_upper_left[1], src_texel_upper_right[1],
                                        src_texel_lower_left[1], src_texel_lower_right[1]])
                        max_b = np.max([src_texel_upper_left[1], src_texel_upper_right[1],
                                        src_texel_lower_left[1], src_texel_lower_right[1]])

                        color = np.zeros([3], dtype=np.uint8)
                        num_texels = 0
                        for a in range(np.ceil(min_a), np.floor(max_a) + 1):
                            for b in range(np.ceil(min_b), np.floor(max_b) + 1):
                                if is_point_in_triangle(
                                        np.array([a, b]), src_texel_upper_left, src_texel_lower_left,
                                        src_texel_upper_right) or is_point_in_triangle(
                                        np.array([a, b]), src_texel_lower_left, src_texel_upper_right,
                                        src_texel_lower_right):
                                    if a < np.shape(image)[1] and b < a < np.shape(image)[0]:
                                        num_texels += 1
                                        color += image[b, a].astype(np.uint8)
                        if num_texels > 0:
                            color //= num_texels
                        else:
                            # if no corresponding texel found, nearest neighbour interpolation
                            # calculate corresponding position of dst_coord pixel center in image (src)
                            homogenous_dst_coord = np.array([x, y, 1])
                            src_texel = warp_mat_org_inv.dot(homogenous_dst_coord)

                            if round(src_texel[1]) < np.shape(image)[0] and round(src_texel[0]) < np.shape(image)[1]:
                                y = round(src_texel[1])
                                x = round(src_texel[0])
                                color = image[y, x].astype(np.uint8)
                        isomap[y, x] = np.hstack([color, np.uint8(alpha_value)])
                    # Bilinear mapping: calculate pixel color depending on the four neighbouring texels
                    elif mapping_type == TextureInterpolation['Bilinear']:
                        # calculate corresponding position of dst_coord pixel center in image (src)
                        homogenous_dst_coord = np.array([x, y, 1])
                        src_texel = warp_mat_org_inv.dot(homogenous_dst_coord)

                        # calculate euclidean distances to next 4 texels
                        distance_upper_left = np.linalg.norm(src_texel, np.floor(src_texel))
                        distance_upper_right = np.sqrt(pow(src_texel[0] - np.floor(src_texel[0]), 2) +
                                                       pow(src_texel[1] - np.ceil(src_texel[1]), 2))
                        distance_lower_left = np.sqrt(pow(src_texel[0] - np.ceil(src_texel[0]), 2) +
                                                      pow(src_texel[1] - np.floor(src_texel[1]), 2))
                        distance_lower_right = np.linalg.norm(src_texel, np.ceil(src_texel))

                        # normalise distances that the sum of all distances is 1
                        sum_distances = \
                            distance_lower_left + distance_lower_right + distance_upper_left + distance_upper_right

                        distance_lower_left /= sum_distances
                        distance_lower_right /= sum_distances
                        distance_upper_left /= sum_distances
                        distance_upper_right /= sum_distances

                        # set color depending on distance from next 4 texels
                        # (we map the data from std::array<uint8_t, 3> to an Eigen::Map, then cast that
                        # to float to multiply with the float-scalar distance.)
                        # (this is untested!)
                        color_upper_left = \
                            image[int(np.floor(src_texel[1])), int(np.floor(src_texel[0]))] * distance_upper_left
                        color_upper_right = \
                            image[int(np.floor(src_texel[1])), int(np.ceil(src_texel[0]))] * distance_upper_right
                        color_lower_left = \
                            image[int(np.ceil(src_texel[1])), int(np.floor(src_texel[0]))] * distance_lower_left
                        color_lower_right = \
                            image[int(np.ceil(src_texel[1])), int(np.ceil(src_texel[0]))] * distance_lower_right

                        isomap[y, x, :3] = np.clip(color_upper_left + color_upper_right + color_lower_left +
                                                   color_lower_right, 0, 255).astype(np.uint8)
                        isomap[y, x, 3] = np.uint8(alpha_value)
                    # NearestNeighbour mapping: set color of pixel to color of nearest texel
                    elif mapping_type == TextureInterpolation['NearestNeighbour']:
                        # calculate corresponding position of dst_coord pixel center in image (src)
                        homogenous_dst_coord = np.array([x, y, 1])
                        src_texel = warp_mat_org_inv.dot(homogenous_dst_coord)

                        if round(src_texel[1]) < np.shape(image)[0] and round(src_texel[1]) < np.shape(image)[1] \
                                and round(src_texel[0]) > 0 and round(src_texel[1]) > 0:
                            isomap[y, x, :3] = image[int(round(src_texel[1])), int(round(src_texel[0]))]
                            isomap[y, x, 3] = np.uint8(alpha_value)

    return isomap


def draw_wireframe(image: np.ndarray, mesh: Mesh.Mesh, modelview: np.ndarray, projection: np.ndarray,
                   viewport: np.ndarray, scale: Optional[float] = 1.0,
                   color: Optional[np.ndarray] = np.array([0, 255, 0, 255])) -> None:
    """
    Draws the given mesh as wireframe into the image.

    It does backface culling, i.e. draws only vertices in CCW order.

    :param image: An image to draw into.
    :param mesh: The mesh to draw.
    :param modelview: Model-view matrix to draw the mesh.
    :param projection: Projection matrix to draw the mesh.
    :param viewport: Viewport to draw the mesh.
    :param scale: the scale of picture to show, in proportion.
    :param color: Colour of the mesh to be drawn.
    :return: None
    """
    alpha = 0.5
    img = np.zeros(np.shape(image), dtype=image.dtype)
    for triangle in mesh.tvi:
        p1 = glm.project(mesh.vertices[triangle[0]], modelview, projection, viewport)[:2] * scale
        p2 = glm.project(mesh.vertices[triangle[1]], modelview, projection, viewport)[:2] * scale
        p3 = glm.project(mesh.vertices[triangle[2]], modelview, projection, viewport)[:2] * scale
        if are_vertices_ccw_in_screen_space(p1[:2], p2[:2], p3[:2]):
            cv2.line(img, tuple(p1.astype(int).tolist()), tuple(p2.astype(int).tolist()), tuple(color.tolist()))
            cv2.line(img, tuple(p2.astype(int).tolist()), tuple(p3.astype(int).tolist()), tuple(color.tolist()))
            cv2.line(img, tuple(p3.astype(int).tolist()), tuple(p1.astype(int).tolist()), tuple(color.tolist()))
    cv2.addWeighted(img, alpha, image, 1-alpha, 0, image)
    return None


def draw_wireframe_with_lm(image: np.ndarray, mesh: Mesh.Mesh, modelview: np.ndarray, projection: np.ndarray,
                           viewport: np.ndarray, point_indices: List[int],
                           color: Optional[np.ndarray] = np.array([0, 255, 0, 255])) -> None:
    """
    Draws the given mesh as wireframe into the image.

    It does backface culling, i.e. draws only vertices in CCW order.

    :param image: An image to draw into.
    :param mesh: The mesh to draw.
    :param modelview: Model-view matrix to draw the mesh.
    :param projection: Projection matrix to draw the mesh.
    :param viewport: Viewport to draw the mesh.
    :param point_indices: The points in Mesh that are to be drawn.
    :param color: Colour of the mesh to be drawn.
    :return: None
    """
    alpha = 0.5
    img = np.zeros(np.shape(image), dtype=image.dtype)
    for triangle in mesh.tvi:
        p1 = glm.project(mesh.vertices[triangle[0]], modelview, projection, viewport)[:2]
        p2 = glm.project(mesh.vertices[triangle[1]], modelview, projection, viewport)[:2]
        p3 = glm.project(mesh.vertices[triangle[2]], modelview, projection, viewport)[:2]
        if are_vertices_ccw_in_screen_space(p1[:2], p2[:2], p3[:2]):
            if triangle[0] in point_indices or triangle[1] in point_indices or triangle[2] in point_indices:
                cv2.line(img, tuple(p1.astype(int).tolist()), tuple(p2.astype(int).tolist()), [0, 0, 255, 255])
                cv2.line(img, tuple(p2.astype(int).tolist()), tuple(p3.astype(int).tolist()), [0, 0, 255, 255])
                cv2.line(img, tuple(p3.astype(int).tolist()), tuple(p1.astype(int).tolist()), [0, 0, 255, 255])
            else:
                cv2.line(img, tuple(p1.astype(int).tolist()), tuple(p2.astype(int).tolist()), tuple(color.tolist()))
                cv2.line(img, tuple(p2.astype(int).tolist()), tuple(p3.astype(int).tolist()), tuple(color.tolist()))
                cv2.line(img, tuple(p3.astype(int).tolist()), tuple(p1.astype(int).tolist()), tuple(color.tolist()))
    cv2.addWeighted(img, alpha, image, 1-alpha, 0, image)
    return None


def draw_wireframe_with_depth(
        image: np.ndarray, mesh: Mesh.Mesh, modelview: np.ndarray, projection: np.ndarray, viewport: np.ndarray,
        landmark_mapper: LandmarkMapper.LandmarkMapper or LandmarkMapper.ProfileLandmarkMapper,
        scale: Optional[float] = 1.0,) -> None:
    """
    Draws the given mesh as wireframe into the image.

    It does backface culling, i.e. draws only vertices in CCW order.

    :param image: An image to draw into.
    :param mesh: The mesh to draw.
    :param modelview: Model-view matrix to draw the mesh.
    :param projection: Projection matrix to draw the mesh.
    :param viewport: Viewport to draw the mesh.
    :param landmark_mapper: the landmark mapper of image.
    :param scale: the scale of picture to show, in proportion.
    :return: None
    """
    alpha = 0.5
    img = np.zeros(np.shape(image), dtype=image.dtype)
    depth_max = np.max(mesh.vertices[:, 2])
    depth_min = np.min(mesh.vertices[:, 2])
    colors = (mesh.vertices[:, 2] - depth_min + 1) * 255 / (depth_max - depth_min)
    colors = colors.clip(0, 255).round().astype(int)
    colors = np.vstack([255 - colors, colors, np.zeros(len(colors))]).T
    for triangle in mesh.tvi:
        p1 = glm.project(mesh.vertices[triangle[0]], modelview, projection, viewport)[:2] * scale
        p2 = glm.project(mesh.vertices[triangle[1]], modelview, projection, viewport)[:2] * scale
        p3 = glm.project(mesh.vertices[triangle[2]], modelview, projection, viewport)[:2] * scale
        if are_vertices_ccw_in_screen_space(p1[:2], p2[:2], p3[:2]):
            cv2.line(img, tuple(p1.astype(int).tolist()), tuple(p2.astype(int).tolist()),
                     tuple(colors[triangle[0]].tolist()))
            cv2.line(img, tuple(p2.astype(int).tolist()), tuple(p3.astype(int).tolist()),
                     tuple(colors[triangle[1]].tolist()))
            cv2.line(img, tuple(p3.astype(int).tolist()), tuple(p1.astype(int).tolist()),
                     tuple(colors[triangle[2]].tolist()))

    if type(landmark_mapper) is LandmarkMapper.LandmarkMapper:
        for i in landmark_mapper.landmark_mappings.values():
            p = glm.project(mesh.vertices[i], modelview, projection, viewport)[:2] * scale
            cv2.circle(img, (int(p[0]), int(p[1])), 1, (0, 0, 255), -1)
    else:
        for i in landmark_mapper.right_mapper.values():
            p = glm.project(mesh.vertices[i], modelview, projection, viewport)[:2] * scale
            cv2.circle(img, (int(p[0]), int(p[1])), 1, (0, 0, 255), -1)

    cv2.addWeighted(img, alpha, image, 1 - alpha, 0, image)
    return None


def save_ply(mesh: Mesh.Mesh, path: str, color: Optional[List] = list([255, 255, 255]),
             author: Optional[str] = 'Unknown') -> None:
    print(path)
    num_vertex = len(mesh.vertices)
    num_face = len(mesh.tvi)
    basename = os.path.basename(path)

    ply = open(path + '.ply', 'w')
    ply.write('ply\n')
    ply.write('format ascii 1.0\n')
    ply.write('comment author: ' + author + '\n')
    ply.write('comment object: ' + basename + '\n')
    ply.write('element vertex ' + str(num_vertex) + '\n')
    ply.write('property float x\n')
    ply.write('property float y\n')
    ply.write('property float z\n')
    ply.write('property uchar red\n')
    ply.write('property uchar green\n')
    ply.write('property uchar blue\n')
    ply.write('element face ' + str(num_face) + '\n')
    ply.write('property list uchar int vertex_indices\n')
    ply.write('end_header\n')

    color_str = ' ' + str(int(color[0])) + ' ' + str(int(color[1])) + ' ' + str(int(color[2])) + '\n'

    lines = ''
    for vertices in mesh.vertices:
        lines += str(vertices).replace('[', '').replace(']', '').strip()
        lines += color_str

    for coor in mesh.tvi:
        lines += '3 '
        lines += str(coor).strip('[]').replace(',', '')
        lines += '\n'
    ply.write(lines)
    ply.close()
    return None
